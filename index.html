<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content=" an LLM-based audio-driven 3D co-speech gesture generation framework.">
  <meta name="keywords" content="Co-speech gesture generation, LLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LLM Gesticulator: Leveraging Large Language Models for Scalable and Controllable Co-Speech Gesture Synthesis</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">LLM Gesticulator: Leveraging Large Language Models for Scalable and Controllable Co-Speech Gesture Synthesis</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://haozhoupang.github.io/">Haozhou Pang</a>,</span>
            <span class="author-block">
              Tianwei Ding</a>,</span>
            <span class="author-block">
              Lanshan He,
            </span>
            <span class="author-block">
             Ming Tao,
            </span>
            <span class="author-block">
              Lu Zhang,
            </span>
            <span class="author-block">
              Qi Gan<sup>*</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Soul AI,  Soulgate Technology Co., Ltd., Shanghai, China</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2410.10851"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.10851"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data (Coming soon)</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/banner.png"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf"> LLM gesticulator</span> synthesizes full-body co-speech gestures according to input audio and text prompt.
      </h2>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In this work, we present LLM Gesticulator, an LLM-based audio-driven co-speech gesture generation framework
            that synthesizes full-body animations that are rhythmically aligned with the input audio while exhibiting natural
            movements and editability. Compared to previous work, our model demonstrates substantial scalability. As the
            size of the backbone LLM model increases, our framework shows proportional improvements in evaluation metrics
            (a.k.a. scaling law). Our method also exhibits strong controllability where the content, style of the generated
            gestures can be controlled by text prompt. To the best of our knowledge, LLM gesticulator is the first work that
            use LLM on the co-speech generation task. Evaluation with existing objective metrics and user studies indicate
            that our framework outperforms prior works.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{pang2024llmgesticulatorleveraginglarge,
      title={LLM Gesticulator: Leveraging Large Language Models for Scalable and Controllable Co-Speech Gesture Synthesis}, 
      author={Haozhou Pang and Tianwei Ding and Lanshan He and Ming Tao and Lu Zhang and Qi Gan},
      year={2024},
      eprint={2410.10851},
      archivePrefix={arXiv},
      primaryClass={cs.GR},
      url={https://arxiv.org/abs/2410.10851}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <!-- <div class="columns is-centered"> -->
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This website was built based on the  <a
              href="https://nerfies.github.io/">Nerfies</a> project page.
          </p>
        </div>
      <!-- </div> -->
    </div>
  </div>
</footer>

</body>
</html>
